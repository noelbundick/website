<!doctype html><html lang=en-us><head><title>Frankenetes! Running the Kubernetes control plane on Azure Container Instances | Noel Bundick</title><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="I&rsquo;ve been learning more about Kubernetes lately - both how to use it, and how it works. I recently took the time to run through Kelsey Hightower&rsquo;s Kubernetes the Hard Way, specifically, the Azure version by Ivan Fioravanti. In addition to learning a lot, it sparked some interesting ideas on my flight home&mldr;
My thought was that these are just apps, processes, binaries that run with flags - boring. Boring is good!"><meta name=generator content="Hugo 0.96.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/css/custom.css><link rel=stylesheet href=/fontawesome/css/fontawesome.min.css><link rel=stylesheet href=/fontawesome/css/brands.min.css><link rel="shortcut icon" href=/images/favicon.ico type=image/x-icon></head><body><nav class=navigation><a href=/><span class=arrow>‚Üê</span>Home</a>
<a href=/posts>Archive</a>
<a href=/tags>Tags</a></nav><main class=main><section id=single><h1 class=title>Frankenetes! Running the Kubernetes control plane on Azure Container Instances</h1><div class=tip><time datetime="2018-01-21 23:02:43 +0000 UTC">Jan 21, 2018</time>
<span class=split>¬∑</span>
<span>1411 words</span>
<span class=split>¬∑</span>
<span>7 minute read</span></div><aside class=toc><details><summary>Table of Contents</summary><div><nav id=TableOfContents><ul><li><a href=#overview>Overview</a></li><li><a href=#prerequisites-azure>Prerequisites: Azure</a></li><li><a href=#etcd>etcd</a><ul><li><a href=#write-ahead-log>Write ahead log</a></li><li><a href=#dns>DNS</a></li></ul></li><li><a href=#apiserver>apiserver</a></li><li><a href=#controller-manager>Controller manager</a></li><li><a href=#scheduler>Scheduler</a></li><li><a href=#kubeconfig>kubeconfig</a></li><li><a href=#nodes>Nodes!</a></li><li><a href=#whats-next>What&rsquo;s next?</a><ul><li><a href=#tls>TLS</a></li><li><a href=#dns-1>DNS</a></li><li><a href=#virtual-kubelet>Virtual Kubelet</a></li></ul></li></ul></nav></div></details></aside><div class=content><p>I&rsquo;ve been learning more about Kubernetes lately - both how to use it, and how it works. I recently took the time to run through Kelsey Hightower&rsquo;s <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way target=_blank rel=noopener>Kubernetes the Hard Way</a>, specifically, the <a href=https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure target=_blank rel=noopener>Azure version</a> by Ivan Fioravanti. In addition to learning a lot, it sparked some interesting ideas on my flight home&mldr;</p><p>My thought was that these are just apps, processes, binaries that run with flags - boring. Boring is good! They all have prebuilt images that let you run them as containers. And hey&mldr; <a href=https://docs.microsoft.com/en-us/azure/container-instances/ target=_blank rel=noopener>Azure Container Instances</a> lets me run containers without VM&rsquo;s. All the hard work was done; I wanted to see if I could glue it together to create a &ldquo;virtual Kubernetes cluster&rdquo;&mldr;</p><p>But I also knew there were already plenty of <a href=https://github.com/Azure/acs-engine target=_blank rel=noopener>sane</a> <a href=https://docs.microsoft.com/en-us/azure/aks/ target=_blank rel=noopener>ways</a> to run a Kubernetes cluster on Azure. And this felt like I was cobbling together parts from all over&mldr; and that if anyone did this in production, it could turn into a monster! So I decided to give my monstrous creation a name.</p><p><p class=markdown-image><img src=/posts/frankenetes-running-the-kubernetes-control-plane-on-azure-container-instances/frankenstein.jpg alt="Boris Karloff as the Kubernetes, I mean Frankenstein monster"></p></p><blockquote><p>Yes, I know that&rsquo;s actually Frankenstein&rsquo;s monster. Don&rsquo;t worry, your nits have been recorded.</p></blockquote><p>Frankenetes! I&rsquo;ll walk you through what I did and some of the gotchas, so you can run your own virtual Kubernetes cluster on ACI</p><h2 id=overview>Overview <a href=#overview class=anchor>üîó</a></h2><p>The Kubernetes control plane consists of a few moving parts</p><ul><li><a href=https://coreos.com/etcd/ target=_blank rel=noopener>etcd</a>: the distributed key/value store that holds cluster data</li><li>apiserver: REST API that validates & controls all reads/writes to etcd</li><li>controller manager: runs the core control loops of Kubernetes</li><li>scheduler: updates pods in the apiserver & assigns them to nodes</li></ul><p>I figured once those were in place, I should be able to connect nodes to my cluster and schedule workloads.</p><blockquote><p>This is a total hack, so I didn&rsquo;t secure <strong>anything</strong>. I don&rsquo;t see a reason why you wouldn&rsquo;t be able to secure all of this with TLS - it just wasn&rsquo;t the initial goal.</p></blockquote><h2 id=prerequisites-azure>Prerequisites: Azure <a href=#prerequisites-azure class=anchor>üîó</a></h2><p>I set up a couple of things in Azure to get started. A resource group to hold everything, and a storage account for all of my persistent data. Right now, it&rsquo;s mainly etcd data, but in the future, I can also store certs, logs, and so on.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#268bd2>AZURE_RESOURCE_GROUP</span><span style=color:#719e07>=</span>frankenetes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75># As of 2.0.25, you have to use &#39;export&#39; for it to automagically set your storage acct</span>
</span></span><span style=display:flex><span><span style=color:#586e75># https://github.com/Azure/azure-cli/issues/5358</span>
</span></span><span style=display:flex><span><span style=color:#b58900>export</span> <span style=color:#268bd2>AZURE_STORAGE_ACCOUNT</span><span style=color:#719e07>=</span>frankenetes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>az group create -n <span style=color:#268bd2>$AZURE_RESOURCE_GROUP</span> -l eastus
</span></span><span style=display:flex><span>az storage account create -n <span style=color:#268bd2>$AZURE_STORAGE_ACCOUNT</span> -g <span style=color:#268bd2>$AZURE_RESOURCE_GROUP</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#268bd2>AZURE_STORAGE_KEY</span><span style=color:#719e07>=</span><span style=color:#719e07>$(</span>az storage account keys list -n <span style=color:#268bd2>$AZURE_STORAGE_ACCOUNT</span> -g <span style=color:#268bd2>$AZURE_RESOURCE_GROUP</span> --query <span style=color:#2aa198>&#39;[0].value&#39;</span> -o tsv<span style=color:#719e07>)</span>
</span></span></code></pre></div><p><p class=markdown-image><img src=/posts/frankenetes-running-the-kubernetes-control-plane-on-azure-container-instances/young_frankenstein.jpg alt="Maybe I should have named it Abby Normal"></p></p><h2 id=etcd>etcd <a href=#etcd class=anchor>üîó</a></h2><p>As the backing data store, etcd needed to come up first. Kubernetes the Hard Way had most of the flags I needed, and the help for <code>az container create</code> gave me everything I needed to map my <code>data-dir</code> to an Azure File share.</p><h3 id=write-ahead-log>Write ahead log <a href=#write-ahead-log class=anchor>üîó</a></h3><p>Unfortunately, I ran into some unusual issues on startup. My limited understanding is that etcd takes a lock on the write ahead log directory, then renames a temp file to boot up. This works fine inside the container, but when mapped to Azure Files, it can&rsquo;t perform the rename (due to the lock), fails, and then the container gets stuck in a crash loop. Setting the <code>wal-dir</code> path to something inside the container resolves the issue, but since the log no longer outlives the container, the data store can suffer data loss.</p><blockquote><p>This could likely be solved if ACI supported additional volume types, etcd was patched to work around the locking issue, etc. Like I said, this is a hack&mldr;</p></blockquote><h3 id=dns>DNS <a href=#dns class=anchor>üîó</a></h3><p>My next issue was the <code>advertise-client-urls</code>. ACI gave me a public IP from, but it was dynamically generated and I wasn&rsquo;t able to specify it or know it until <strong>after</strong> the container group had been created. I needed another level of abstraction - DNS to the rescue! I&rsquo;m listening on all addresses inside the container, and then I configured DNS to let the apiserver resolve my etcd host from outside the container.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#586e75># Create an Azure File share to hold cluster data</span>
</span></span><span style=display:flex><span>az storage share create -n etcd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75>#WARNING! This isn&#39;t truly useful until I fix the write ahead log &amp; secure the cluster</span>
</span></span><span style=display:flex><span><span style=color:#586e75>#TODO: figure out why I get &#34;create wal error: rename /etcd/data/member/wal.tmp /etcd/data/member/wal: permission denied&#34; when --wal-dir is not set</span>
</span></span><span style=display:flex><span>az container create -g <span style=color:#268bd2>$AZURE_RESOURCE_GROUP</span> <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --name etcd <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --image quay.io/coreos/etcd:v3.2.8 <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --azure-file-volume-account-name <span style=color:#268bd2>$AZURE_STORAGE_ACCOUNT</span> <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --azure-file-volume-account-key <span style=color:#268bd2>$AZURE_STORAGE_KEY</span> <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --azure-file-volume-share-name etcd <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --azure-file-volume-mount-path /etcd <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --ports <span style=color:#2aa198>2379</span> <span style=color:#2aa198>2389</span> <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --ip-address public <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --command-line <span style=color:#2aa198>&#39;/usr/local/bin/etcd --name=aci --data-dir=/etcd/data --wal-dir=/etcd-wal --listen-client-urls=http://0.0.0.0:2379 --advertise-client-urls=http://frankenetes-etcd.noelbundick.com:2379&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75># Grab the ipAddress.ip property &amp; update the A record for &#39;frankenetes-etcd.noelbundick.com&#39;</span>
</span></span></code></pre></div><p>And here&rsquo;s where I updated my DNS on CloudFlare</p><p><p class=markdown-image><img src=/posts/frankenetes-running-the-kubernetes-control-plane-on-azure-container-instances/frankenetes-etcd-dns.png alt="Adding the etcd record in CloudFlare DNS"></p></p><p>Finally, I verified everything by running a few etcdctl commands against the remote <code>frankenetes-etcd.noelbundick.com:2379</code> host</p><h2 id=apiserver>apiserver <a href=#apiserver class=anchor>üîó</a></h2><p>Next up - the apiserver. I used the latest stable <a href=https://github.com/kubernetes/kubernetes/tree/master/cluster/images/hyperkube target=_blank rel=noopener>hyperkube</a> image to run the API server, and connected to etcd using the DNS name.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#586e75># Create a share to hold logs/etc</span>
</span></span><span style=display:flex><span>az storage share create -n apiserver
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>az container create -g <span style=color:#268bd2>$AZURE_RESOURCE_GROUP</span> <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --name apiserver <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --image gcr.io/google-containers/hyperkube-amd64:v1.9.2 <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --azure-file-volume-account-name <span style=color:#268bd2>$AZURE_STORAGE_ACCOUNT</span> <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --azure-file-volume-account-key <span style=color:#268bd2>$AZURE_STORAGE_KEY</span> <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --azure-file-volume-share-name apiserver <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --azure-file-volume-mount-path /apiserverdata <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --ports <span style=color:#2aa198>6445</span> <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --ip-address public <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --command-line <span style=color:#2aa198>&#39;/apiserver  --advertise-address=0.0.0.0 --allow-privileged=true --apiserver-count=1 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/apiserverdata/log/audit.log --authorization-mode=Node,RBAC --bind-address=0.0.0.0 --etcd-servers=http://frankenetes-etcd.noelbundick.com:2379 --runtime-config=api/all --v=2 --runtime-config=admissionregistration.k8s.io/v1alpha1 --enable-swagger-ui=true --event-ttl=1h --service-node-port-range=30000-32767 --insecure-bind-address=0.0.0.0 --insecure-port 6445&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75># Grab the ipAddress.ip property &amp; update the A record for &#39;frankenetes-apiserver.noelbundick.com&#39;</span>
</span></span></code></pre></div><blockquote><p>Running on 6445 instead of 6443 was me &ldquo;securing&rdquo; my cluster by not using a default port. Security through obscurity is dumb. Don&rsquo;t do what I did for anything that matters</p></blockquote><p>And one more DNS update for the apiserver</p><p><p class=markdown-image><img src=/posts/frankenetes-running-the-kubernetes-control-plane-on-azure-container-instances/frankenetes-apiserver-dns.png alt="Adding the apiserver record in CloudFlare DNS"></p></p><p>To verify, I hit the apiserver endpoint by running <code>curl http://frankenetes-apiserver.noelbundick.com:6445/version</code></p><h2 id=controller-manager>Controller manager <a href=#controller-manager class=anchor>üîó</a></h2><p>Cool! The hard part was done. Smooth sailing from here on out. The controller manager was pretty straightforward - I just pointed it at the apiserver. No certs, no problems!</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>az container create -g <span style=color:#268bd2>$AZURE_RESOURCE_GROUP</span> <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --name controllermanager <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --image gcr.io/google-containers/hyperkube-amd64:v1.9.2 <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --command-line <span style=color:#2aa198>&#39;/controller-manager --address=0.0.0.0 --cluster-cidr=10.200.0.0/16 --cluster-name=kubernetes --leader-elect=true --master=http://frankenetes-apiserver.noelbundick.com:6445 --service-cluster-ip-range=10.32.0.0/24 --v=2&#39;</span>
</span></span></code></pre></div><h2 id=scheduler>Scheduler <a href=#scheduler class=anchor>üîó</a></h2><p>Same story for the scheduler - it just needs to know where the apiserver is. Around this point, I really started to appreciate how modular the Kubernetes core components were.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>az container create -g <span style=color:#268bd2>$AZURE_RESOURCE_GROUP</span> <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --name scheduler <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --image gcr.io/google-containers/hyperkube-amd64:v1.9.2 <span style=color:#cb4b16>\
</span></span></span><span style=display:flex><span><span style=color:#cb4b16></span>  --command-line <span style=color:#2aa198>&#39;/scheduler --leader-elect=true --master=http://frankenetes-apiserver.noelbundick.com:6445 --v=2&#39;</span>
</span></span></code></pre></div><h2 id=kubeconfig>kubeconfig <a href=#kubeconfig class=anchor>üîó</a></h2><p>I needed a way to connect to my cobbled together Kubernetes cluster (still with no nodes). I used the following to set up my kubeconfig and verify everything was working.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#586e75># Set up cluster/context info in a standalone file</span>
</span></span><span style=display:flex><span>kubectl config set-cluster frankenetes --server<span style=color:#719e07>=</span>http://frankenetes-apiserver.noelbundick.com:6445 --kubeconfig<span style=color:#719e07>=</span>frankenetes.kubeconfig
</span></span><span style=display:flex><span>kubectl config set-context default --cluster<span style=color:#719e07>=</span>frankenetes --kubeconfig<span style=color:#719e07>=</span>frankenetes.kubeconfig
</span></span><span style=display:flex><span>kubectl config use-context default --kubeconfig<span style=color:#719e07>=</span>frankenetes.kubeconfig
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#586e75># Use the kubeconfig &amp; cross your fingers!</span>
</span></span><span style=display:flex><span><span style=color:#b58900>export</span> <span style=color:#268bd2>KUBECONFIG</span><span style=color:#719e07>=</span>frankenetes.kubeconfig
</span></span><span style=display:flex><span>kubectl version
</span></span><span style=display:flex><span>kubectl api-versions
</span></span></code></pre></div><p><p class=markdown-image><img src=/posts/frankenetes-running-the-kubernetes-control-plane-on-azure-container-instances/young_frankenstein_2.jpg alt="Frankenetes is alive"></p></p><h2 id=nodes>Nodes! <a href=#nodes class=anchor>üîó</a></h2><p>I ran through the manual steps in Kubernetes the Hard Way on Azure to create a single node, stripping out all the TLS bits along the way, and substituting 1.9.2 for 1.8.0.</p><p>Relevant sections:</p><ul><li><a href=https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure/blob/master/docs/03-compute-resources.md target=_blank rel=noopener>Compute resources</a> - Virtual Network, Firewall Rules, Kubernetes Workers</li><li><a href=https://github.com/ivanfioravanti/kubernetes-the-hard-way-on-azure/blob/master/docs/09-bootstrapping-kubernetes-workers.md target=_blank rel=noopener>Bootstrapping the Kubernetes Worker Nodes</a></li></ul><p>After all was said & done, I was able to run pods on my node, all being controlled by Frankenetes running in ACI!</p><p><p class=markdown-image><img src=/posts/frankenetes-running-the-kubernetes-control-plane-on-azure-container-instances/pods.jpg alt="Lots of work to make it useful, but it does work!"></p></p><h2 id=whats-next>What&rsquo;s next? <a href=#whats-next class=anchor>üîó</a></h2><p>Not sure! This was a learning project to understand more about how the Kubernetes control plane really worked. It doesn&rsquo;t seem mysterious anymore. I feel like I can make it do what I want now. Here are some of my ideas so far. If you&rsquo;ve got more, please <a href=https://twitter.com/acanthamoeba target=_blank rel=noopener>hit me up on Twitter</a>!</p><h3 id=tls>TLS <a href=#tls class=anchor>üîó</a></h3><p>This is horribly insecure. Wide open. I should be able to use the DNS names to generate proper certs and secure everything. And then, of course, automate it all away.</p><h3 id=dns-1>DNS <a href=#dns-1 class=anchor>üîó</a></h3><p>I want to automate all the DNS steps using <a href=https://docs.microsoft.com/en-us/azure/event-grid/ target=_blank rel=noopener>Azure Event Grid</a>. My plan is to create the container group with a tag containing my desired DNS name (needs a PR for azure-cli!) Then, I can subscribe to Azure Resource Manager events and fire off an Azure Function that will take care of updating my DNS records based on the tag and the IP address that was provisioned. Think of it as a kind of Frankenstein version of kube-dns!</p><h3 id=virtual-kubelet>Virtual Kubelet <a href=#virtual-kubelet class=anchor>üîó</a></h3><p>Why stop at a virtual master control plane when I could have virtual nodes! I want to wire up <a href=https://github.com/virtual-kubelet/virtual-kubelet target=_blank rel=noopener>virtual-kubelet</a>. If that works, I could interact with my virtual cluster with my familiar Kubernetes toolkit (kubectl, kubectx, kubens, etc), but then <strong>all</strong> of my workloads would run on ACI. Not sure that&rsquo;s a useful concept, but hey - I&rsquo;ll give it a try and see if anyone likes it!</p><p><p class=markdown-image><img src=/posts/frankenetes-running-the-kubernetes-control-plane-on-azure-container-instances/young_frankenstein_3.gif alt="Me, when I got everything up and running"></p></p></div><div class=tags><a href=/tags/azure>azure</a>
<a href=/tags/kubernetes>kubernetes</a>
<a href=/tags/aci>aci</a></div></section></main><section><ul class=post-list><li>2021-10-14
<a href=/gists/optimizing-rust-container-builds/>Optimizing Rust container builds<h2></h2></a></li><li>2021-09-19
<a href=/gists/wsl2-container-development-with-moby/>WSL2 container development with Moby<h2></h2></a></li><li>2019-12-17
<a href=/gists/rdp-from-wsl/>RDP from WSL<h2></h2></a></li><li>2019-06-28
<a href=/gists/how-to-use-docker-build-secrets/>How to use Docker build secrets<h2></h2></a></li><li>2019-06-12
<a href=/gists/consuming-packages-from-a-private-azure-pipelines-python-artifact-feed/>Consuming packages from a private Azure Pipelines Python artifact feed<h2></h2></a></li><li>2019-01-13
<a href=/gists/gists-as-a-content-management-system/>Gists as a content management system<h2></h2></a></li><li>2019-01-12
<a href=/gists/azure-function-w--user-assigned-identity/>Azure Function w/ User Assigned Identity<h2></h2></a></li><li>2018-12-15
<a href=/gists/secure-code-execution-via-arm-template-and-azure-container-instances/>Secure code execution via ARM template and Azure Container Instances<h2></h2></a></li><li>2018-12-10
<a href=/gists/azure-redis-cli/>azure-redis-cli<h2></h2></a></li><li>2018-12-10
<a href=/gists/resize-azure-cloud-shell-storage/>Resize Azure Cloud Shell storage<h2></h2></a></li></ul></section><footer id=footer><div id=social><a class=symbol href=https://www.github.com/noelbundick><i class="fa-brands fa-github"></i></a>
<a class=symbol href=https://www.linkedin.com/in/noelbundick/><i class="fa-brands fa-linkedin"></i></a>
<a class=symbol href=https://twitter.com/acanthamoeba><i class="fa-brands fa-twitter"></i></a></div><div class=copyright>¬© Copyright
2022
<span class=split><svg fill="#bbb" width="15" height="15" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15" height="15" viewBox="0 0 15 15"><path d="M13.91 6.75c-1.17 2.25-4.3 5.31-6.07 6.94-.1903.1718-.4797.1718-.67.0C5.39 12.06 2.26 9 1.09 6.75-1.48 1.8 5-1.5 7.5 3.45 10-1.5 16.48 1.8 13.91 6.75z"/></svg></span>Noel Bundick</div></footer></body></html>